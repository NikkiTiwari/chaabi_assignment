{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\", device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"bigBasketProducts.csv\")\n",
    "df[\"rating\"]=df[\"rating\"].astype(str)\n",
    "df[\"rating\"].replace({'nan': 'no rating', None: 'no rating'}, inplace=True)\n",
    "df[\"description\"].fillna(\"no desc\",inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch no-1 competed\n",
      "batch no-2 competed\n",
      "batch no-3 competed\n",
      "batch no-4 competed\n",
      "batch no-5 competed\n",
      "batch no-6 competed\n",
      "batch no-7 competed\n",
      "batch no-8 competed\n",
      "batch no-9 competed\n",
      "batch no-10 competed\n",
      "batch no-11 competed\n",
      "batch no-12 competed\n",
      "batch no-13 competed\n",
      "batch no-14 competed\n",
      "batch no-15 competed\n",
      "batch no-16 competed\n",
      "batch no-17 competed\n",
      "batch no-18 competed\n",
      "batch no-19 competed\n",
      "batch no-20 competed\n",
      "batch no-21 competed\n",
      "batch no-22 competed\n",
      "batch no-23 competed\n",
      "batch no-24 competed\n",
      "batch no-25 competed\n",
      "batch no-26 competed\n",
      "batch no-27 competed\n",
      "batch no-28 competed\n",
      "batch no-29 competed\n",
      "batch no-30 competed\n",
      "batch no-31 competed\n",
      "batch no-32 competed\n",
      "batch no-33 competed\n",
      "batch no-34 competed\n",
      "batch no-35 competed\n",
      "batch no-36 competed\n",
      "batch no-37 competed\n",
      "batch no-38 competed\n",
      "batch no-39 competed\n",
      "batch no-40 competed\n",
      "batch no-41 competed\n",
      "batch no-42 competed\n",
      "batch no-43 competed\n",
      "batch no-44 competed\n",
      "batch no-45 competed\n",
      "batch no-46 competed\n",
      "batch no-47 competed\n",
      "batch no-48 competed\n",
      "batch no-49 competed\n",
      "batch no-50 competed\n",
      "batch no-51 competed\n",
      "batch no-52 competed\n",
      "batch no-53 competed\n",
      "batch no-54 competed\n",
      "batch no-55 competed\n",
      "batch no-56 competed\n",
      "batch no-57 competed\n",
      "batch no-58 competed\n",
      "batch no-59 competed\n",
      "batch no-60 competed\n",
      "batch no-61 competed\n",
      "batch no-62 competed\n",
      "batch no-63 competed\n",
      "batch no-64 competed\n",
      "batch no-65 competed\n",
      "batch no-66 competed\n",
      "batch no-67 competed\n",
      "batch no-68 competed\n",
      "batch no-69 competed\n",
      "batch no-70 competed\n",
      "batch no-71 competed\n",
      "batch no-72 competed\n",
      "batch no-73 competed\n",
      "batch no-74 competed\n",
      "batch no-75 competed\n",
      "batch no-76 competed\n",
      "batch no-77 competed\n",
      "batch no-78 competed\n",
      "batch no-79 competed\n",
      "batch no-80 competed\n",
      "batch no-81 competed\n",
      "batch no-82 competed\n",
      "batch no-83 competed\n",
      "batch no-84 competed\n",
      "batch no-85 competed\n",
      "batch no-86 competed\n",
      "batch no-87 competed\n",
      "batch no-88 competed\n",
      "batch no-89 competed\n",
      "batch no-90 competed\n",
      "batch no-91 competed\n",
      "batch no-92 competed\n",
      "batch no-93 competed\n",
      "batch no-94 competed\n",
      "batch no-95 competed\n",
      "batch no-96 competed\n",
      "batch no-97 competed\n",
      "batch no-98 competed\n",
      "batch no-99 competed\n",
      "batch no-100 competed\n",
      "batch no-101 competed\n",
      "batch no-102 competed\n",
      "batch no-103 competed\n",
      "batch no-104 competed\n",
      "batch no-105 competed\n",
      "batch no-106 competed\n",
      "batch no-107 competed\n",
      "batch no-108 competed\n",
      "batch no-109 competed\n",
      "batch no-110 competed\n",
      "batch no-111 competed\n",
      "batch no-112 competed\n",
      "batch no-113 competed\n",
      "batch no-114 competed\n",
      "batch no-115 competed\n",
      "batch no-116 competed\n",
      "batch no-117 competed\n",
      "batch no-118 competed\n",
      "batch no-119 competed\n",
      "batch no-120 competed\n",
      "batch no-121 competed\n",
      "batch no-122 competed\n",
      "batch no-123 competed\n",
      "batch no-124 competed\n",
      "batch no-125 competed\n",
      "batch no-126 competed\n",
      "batch no-127 competed\n",
      "batch no-128 competed\n",
      "batch no-129 competed\n",
      "batch no-130 competed\n",
      "batch no-131 competed\n",
      "batch no-132 competed\n",
      "batch no-133 competed\n",
      "batch no-134 competed\n",
      "batch no-135 competed\n",
      "batch no-136 competed\n",
      "batch no-137 competed\n",
      "batch no-138 competed\n",
      "batch no-139 competed\n",
      "batch no-140 competed\n",
      "batch no-141 competed\n",
      "batch no-142 competed\n",
      "batch no-143 competed\n",
      "batch no-144 competed\n",
      "batch no-145 competed\n",
      "batch no-146 competed\n",
      "batch no-147 competed\n",
      "batch no-148 competed\n",
      "batch no-149 competed\n",
      "batch no-150 competed\n",
      "batch no-151 competed\n",
      "batch no-152 competed\n",
      "batch no-153 competed\n",
      "batch no-154 competed\n",
      "batch no-155 competed\n",
      "batch no-156 competed\n",
      "batch no-157 competed\n",
      "batch no-158 competed\n",
      "batch no-159 competed\n",
      "batch no-160 competed\n",
      "batch no-161 competed\n",
      "batch no-162 competed\n",
      "batch no-163 competed\n",
      "batch no-164 competed\n",
      "batch no-165 competed\n",
      "batch no-166 competed\n",
      "batch no-167 competed\n",
      "batch no-168 competed\n",
      "batch no-169 competed\n",
      "batch no-170 competed\n",
      "batch no-171 competed\n",
      "batch no-172 competed\n",
      "batch no-173 competed\n",
      "batch no-174 competed\n",
      "batch no-175 competed\n",
      "batch no-176 competed\n",
      "batch no-177 competed\n",
      "batch no-178 competed\n",
      "batch no-179 competed\n",
      "batch no-180 competed\n",
      "batch no-181 competed\n",
      "batch no-182 competed\n",
      "batch no-183 competed\n",
      "batch no-184 competed\n",
      "batch no-185 competed\n",
      "batch no-186 competed\n",
      "batch no-187 competed\n",
      "batch no-188 competed\n",
      "batch no-189 competed\n",
      "batch no-190 competed\n",
      "batch no-191 competed\n",
      "batch no-192 competed\n",
      "batch no-193 competed\n",
      "batch no-194 competed\n",
      "batch no-195 competed\n",
      "batch no-196 competed\n",
      "batch no-197 competed\n",
      "batch no-198 competed\n",
      "batch no-199 competed\n",
      "batch no-200 competed\n",
      "batch no-201 competed\n",
      "batch no-202 competed\n",
      "batch no-203 competed\n",
      "batch no-204 competed\n",
      "batch no-205 competed\n",
      "batch no-206 competed\n",
      "batch no-207 competed\n",
      "batch no-208 competed\n",
      "batch no-209 competed\n",
      "batch no-210 competed\n",
      "batch no-211 competed\n",
      "batch no-212 competed\n",
      "batch no-213 competed\n",
      "batch no-214 competed\n",
      "batch no-215 competed\n",
      "batch no-216 competed\n"
     ]
    }
   ],
   "source": [
    "class make_embedding_ds(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, csv = df):\n",
    "        \"\"\"\n",
    "        will compute embedding using sentence encoder\n",
    "        \n",
    "        \"\"\"\n",
    "        super(make_embedding_ds).__init__()\n",
    "        self.csv = csv\n",
    "        self.total_row = csv.shape[0]\n",
    "        self.col = csv.columns.to_list()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for row_no in range(self.total_row):\n",
    "            \n",
    "            #testing: make comment it later\n",
    "       \n",
    "            \n",
    "            \n",
    "            row = self.csv.iloc[row_no].to_dict()\n",
    "            product_name = row[self.col[0]]\n",
    "            story = f\"{row[ self.col[0] ]} is of category {row[self.col[1]]} and sub category is {row[self.col[2]]}. {row[self.col[0]]} is type {row[self.col[6]]}. brand of {row[self.col[0]]} is {row[self.col[3]]}, with rating {row[self.col[7]]}. sale price of {row[self.col[0]]} is {row[self.col[4]]} with market price {row[self.col[5]]}, description of {row[self.col[0]]} is {row[self.col[8]]}\"\n",
    "            emb = model.encode(story)\n",
    "            \n",
    "            \n",
    "            yield row_no,product_name,story,emb\n",
    "    \n",
    "\n",
    "ds = make_embedding_ds(df)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(ds, num_workers=1,batch_size=128)\n",
    "json_emb = {\n",
    "    \"payload\":[],\n",
    "    \"emb\": []\n",
    "}\n",
    "batch_no = 1\n",
    "for row,product_name,story,emb in dataloader:\n",
    "    \n",
    "    _batch_len = len(row)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"batch no-{batch_no} competed\")\n",
    "    batch_no += 1\n",
    "    \n",
    "    for index in range(_batch_len):\n",
    "        \n",
    "        temp = {}\n",
    "        temp[\"id\"] = row[index].item()\n",
    "        temp[\"product\"] = product_name[index]\n",
    "        temp[\"story\"] = story[index]\n",
    "        \n",
    "        json_emb[\"payload\"].append(temp)\n",
    "        json_emb[\"emb\"].append(emb[index].numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Deserialize the data from the pickle file\n",
    "with open('space.pkl', 'wb') as file:\n",
    "    pickle.dump(json_emb, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import pipeline\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='qdrant-space')]\n",
      "Vector count in collection: 27553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473c1685ff904a3b92a88713c60f1ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VECTOR_SPACE_PATH = \"space.pkl\"\n",
    "with open(VECTOR_SPACE_PATH, 'rb') as file:\n",
    "    vs = pickle.load(file)\n",
    "client = QdrantClient(\":memory:\")\n",
    "collection_name = \"qdrant-space\"\n",
    "\n",
    "collections = client.get_collections()\n",
    "if collection_name not in [c.name for c in collections.collections]:\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=384,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    )\n",
    "print(client.get_collections())\n",
    "\n",
    "total_records = len(vs[\"payload\"]) # total records data\n",
    "_payload = vs[\"payload\"]\n",
    "_emb = vs[\"emb\"]\n",
    "ids = list(range(0,total_records))\n",
    "\n",
    "batch_size = 2  # specify batch size according to your RAM and compute, higher batch size = more RAM usage\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=models.Batch(ids=ids, vectors=_emb, payloads=_payload),\n",
    ")\n",
    "\n",
    "\n",
    "collection_vector_count = client.get_collection(collection_name=collection_name).vectors_count\n",
    "print(f\"Vector count in collection: {collection_vector_count}\")\n",
    "# dump qdrant space note keep sentence transformer in environment \n",
    "\n",
    "with open('qdrant_space_client.pkl', 'wb') as file:\n",
    "    pickle.dump(client, file)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\", device=device\n",
    ")\n",
    "\n",
    "with open('encodermodel.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "reader = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
    "with open('bert-question-answering.pkl', 'wb') as file:\n",
    "    pickle.dump(reader, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY INPUT: suggest me one product for cleaning vegetables\n",
      "OUTPUT: Washing Liquid \n",
      "PREDICTION SCORE 0.17999418079853058\n",
      "\n",
      "Referred Product: 14873\n",
      "\n",
      "\n",
      "\n",
      "QUERY INPUT: what is the rating of product Vegetable & Fruit Wash with 100% Natural Action\n",
      "OUTPUT: Disinfectant Spray & Cleaners \n",
      "PREDICTION SCORE 0.03999422863125801\n",
      "\n",
      "Referred Product: 14873\n",
      "\n",
      "\n",
      "\n",
      "QUERY INPUT: what is most loved beauty product\n",
      "OUTPUT: Love Beauty & Planet \n",
      "PREDICTION SCORE 0.11456327140331268\n",
      "\n",
      "Referred Product: 11395\n",
      "\n",
      "\n",
      "\n",
      "QUERY INPUT: price of dove soap\n",
      "OUTPUT: sale price of 15677 is Dove with market price 115.0 \n",
      "PREDICTION SCORE 0.31905046105384827\n",
      "\n",
      "Referred Product: 15677\n",
      "\n",
      "\n",
      "\n",
      "QUERY INPUT: what is most loved beauty product\n",
      "OUTPUT: Love Beauty & Planet \n",
      "PREDICTION SCORE 0.11456327140331268\n",
      "\n",
      "Referred Product: 11395\n",
      "\n",
      "\n",
      "\n",
      "QUERY INPUT: suggest one Tea Product\n",
      "OUTPUT: 20684 \n",
      "PREDICTION SCORE 0.16108818352222443\n",
      "\n",
      "Referred Product: 20684\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VECTOR_SPACE_PATH = \"/kaggle/input/chaabi/space.pkl\"\n",
    "BERT_MODEL_PATH = \"bert-question-answering.pkl\"\n",
    "ENC_PATH  = \"encodermodel.pkl\"\n",
    "QDRANT_PATH  = \"qdrant_space_client.pkl\"\n",
    "\n",
    "\n",
    "# with open(VECTOR_SPACE_PATH, 'rb') as file:\n",
    "#     vs = pickle.load(file)\n",
    "# bert llm\n",
    "with open(BERT_MODEL_PATH, 'rb') as file:\n",
    "    bert = pickle.load(file)\n",
    "# qdrant clinet to interact qdrant space containing all vectors \n",
    "\n",
    "with open(QDRANT_PATH, 'rb') as file:\n",
    "    qdrant_client = pickle.load(file)\n",
    "\n",
    "with open(ENC_PATH, 'rb') as file:\n",
    "    st_encoder = pickle.load(file)\n",
    "collection_name = \"qdrant-space\"\n",
    "def find_close_contexts(question: str, top_k: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    will return contexts contexts close to query \n",
    "\n",
    "    Args:\n",
    "        question (str): What do we want to know?\n",
    "        top_k (int): top k results will be added \n",
    "\n",
    "    Returns:\n",
    "        context (List[str]):\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoded_query = st_encoder.encode(question).tolist() \n",
    "        result = qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=encoded_query,\n",
    "            limit=top_k,\n",
    "        )  # search qdrant collection for context passage with the answer\n",
    "\n",
    "        context = [\n",
    "            [context.payload[\"product\"], context.payload[\"story\"]] for context in result\n",
    "        ] \n",
    "        return context\n",
    "    except Exception as e:\n",
    "        print({e})\n",
    "def tell_me(question: str, context: list[str]):\n",
    "    \"\"\"\n",
    "    Extract the answer from the context for a given question\n",
    "\n",
    "    Args:\n",
    "        question (str): _description_\n",
    "        context (list[str]): _description_\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for c in context:\n",
    "        answer = bert(question=question, context=c[1] )\n",
    "        answer[\"product\"] = c[0]\n",
    "        results.append(answer)\n",
    "        print()\n",
    "\n",
    "    sorted_result = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "    for i in range(len(sorted_result)):\n",
    "        _out = sorted_result[i][\"answer\"] \n",
    "        _prod = sorted_result[i][\"product\"]\n",
    "        _sco = sorted_result[i][\"score\"]\n",
    "#         print(f\"{i+1}\", end=\" \")\n",
    "        print(f\"QUERY INPUT: {question}\")\n",
    "        print(f\"OUTPUT: {_out} \\nPREDICTION SCORE {_sco}\\n\\nReferred Product: {_prod}\\n\\n\")\n",
    "        return question,_out,_sco,_prod\n",
    "queries = []\n",
    "queries.append(\"suggest me one product for cleaning vegetables\")\n",
    "queries.append(\"what is the rating of product Vegetable & Fruit Wash with 100% Natural Action\")\n",
    "queries.append(\"what is most loved beauty product\")\n",
    "queries.append(\"price of dove soap\")\n",
    "queries.append(\"what is most loved beauty product\")\n",
    "queries.append(\"suggest one Tea Product\")\n",
    "with open(\"result.txt\",\"w\") as result:\n",
    "    for q in queries:\n",
    "        \n",
    "        c = find_close_contexts(q, top_k=1)\n",
    "        _ques,_out,_sco,_prod = tell_me(q,c)\n",
    "        result.write(f\"QUERY INPUT: {_ques}\\nOUTPUT: {_out} \\nPREDICTION SCORE {_sco}\\n\\nReferded Product: {_prod}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
